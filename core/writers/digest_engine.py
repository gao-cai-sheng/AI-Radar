import os
import json
import logging
from typing import List, Dict, Any, Optional
from openai import OpenAI
from datetime import datetime

logger = logging.getLogger(__name__)

class DigestEngine:
    """
    Generates AI-powered digests/summaries for news feeds.
    é™ç»´å¼•æ“: æŠŠ N æ¡æ–°é—»å‹ç¼©æˆæ ¸å¿ƒè¦ç‚¹ã€‚
    """
    
    def __init__(self):
        api_key = os.getenv('DEEPSEEK_API_KEY')
        if not api_key:
            raise ValueError("Missing DEEPSEEK_API_KEY environment variable")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        self.model = "deepseek-chat"
    
    def generate_news_digest(self, news_items: List[Dict[str, Any]], max_items: int = 15) -> str:
        """
        Takes a list of news items and generates a concise Chinese digest.
        """
        if not news_items:
            return "æš‚æ— æ–°é—»æ•°æ®ï¼Œè¯·å…ˆæŠ“å– News Feedã€‚"
        
        # Prepare context for LLM
        news_text = ""
        for i, item in enumerate(news_items[:max_items]):
            news_text += f"{i+1}. [{item.get('source', 'Unknown')}] {item.get('title', 'No Title')}\n"
            news_text += f"   {item.get('summary', '')[:150]}\n\n"
        
        today = datetime.now().strftime("%Y-%m-%d")
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ {len(news_items[:max_items])} æ¡æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ï¼Œç”Ÿæˆä¸€ä»½ç²¾ç‚¼çš„ "ä»Šæ—¥ AI é€Ÿè§ˆ"ã€‚

è¦æ±‚:
1. ç”¨ä¸­æ–‡è¾“å‡º
2. åˆ†ä¸º "ğŸ”¥ æ ¸å¿ƒåŠ¨æ€" (3-5 æ¡æœ€é‡è¦çš„) å’Œ "ğŸ“Œ å…¶ä»–å€¼å¾—å…³æ³¨" (ç®€çŸ­ä¸€å¥è¯åˆ—è¡¨)
3. æ¯æ¡è¦ç‚¹è¦å½’çº³ï¼Œä¸è¦ç…§æŠ„æ ‡é¢˜
4. å¦‚æœæœ‰å¤šæ¡æ–°é—»è®²åŒä¸€ä»¶äº‹ï¼Œåˆå¹¶ä¸ºä¸€æ¡
5. é£æ ¼ç®€æ´ã€ä¿¡æ¯å¯†åº¦é«˜

æ–°é—»æ•°æ®:
{news_text}

è¯·è¾“å‡ºä»Šæ—¥ AI é€Ÿè§ˆ ({today}):
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a concise AI industry analyst."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.5
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Digest generation failed: {e}")
            return f"Digest ç”Ÿæˆå¤±è´¥: {e}"

    def generate_podcast_digest(self, episodes: List[Dict[str, Any]], max_items: int = 5) -> str:
        """
        Summarizes podcast episodes into key takeaways.
        """
        if not episodes:
            return "æš‚æ— æ’­å®¢æ•°æ®ã€‚"
        
        ep_text = ""
        for i, ep in enumerate(episodes[:max_items]):
            ep_text += f"{i+1}. [{ep.get('source', '')}] {ep.get('title', '')}\n"
            ep_text += f"   å˜‰å®¾: {', '.join(ep.get('guests', ['Unknown']))}\n"
            ep_text += f"   æ‘˜è¦: {ep.get('summary', '')[:100]}\n\n"
        
        prompt = f"""
ä½œä¸º AI é¢†åŸŸçš„è§‚å¯Ÿè€…ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ’­å®¢æ‘˜è¦ï¼Œæç‚¼å‡º "æœ¬å‘¨å¯¹è¯ç²¾å"ã€‚

è¦æ±‚:
1. ç”¨ä¸­æ–‡è¾“å‡º
2. æ¯ä¸ªæ’­å®¢æç‚¼ 1-2 ä¸ªæ ¸å¿ƒè§‚ç‚¹æˆ–"é‡‘å¥"
3. å¦‚æœèƒ½çœ‹å‡ºè¡Œä¸šè¶‹åŠ¿ï¼Œè¯·åœ¨æœ€åæ€»ç»“

æ’­å®¢æ•°æ®:
{ep_text}

è¯·è¾“å‡ºæœ¬å‘¨å¯¹è¯ç²¾å:
"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a podcast summarizer."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=600,
                temperature=0.5
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Podcast digest failed: {e}")
            return f"Podcast Digest ç”Ÿæˆå¤±è´¥: {e}"


if __name__ == "__main__":
    # Test
    engine = DigestEngine()
    test_news = [
        {"source": "OpenAI", "title": "GPT-5 Announced", "summary": "OpenAI unveils next gen model..."},
        {"source": "Google AI", "title": "Gemini 2.0 Flash", "summary": "Google releases faster model..."},
    ]
    print(engine.generate_news_digest(test_news))
